# 模型推理代码编写说明<a name="modelarts_23_0093"></a>

本章节介绍了在ModelArts中模型推理代码编写的通用方法及说明，针对常用AI引擎的自定义脚本代码示例（包含推理代码示例），请参见[自定义脚本代码示例](自定义脚本代码示例.md)。本文在编写说明下方提供了一个TensorFlow引擎的推理代码示例以及一个在推理脚本中自定义推理逻辑的示例。

## 推理代码编写指导<a name="section91713215288"></a>

1.  所有的自定义Python代码必须继承自BaseService类，不同类型的模型父类导入语句如[表1](#table55021545175412)所示。

    **表 1**  BaseService类导入语句

    <a name="table55021545175412"></a>
    <table><thead align="left"><tr id="row8538104512544"><th class="cellrowborder" valign="top" width="13.05%" id="mcps1.2.4.1.1"><p id="p5538104519547"><a name="p5538104519547"></a><a name="p5538104519547"></a>模型类型</p>
    </th>
    <th class="cellrowborder" valign="top" width="20.02%" id="mcps1.2.4.1.2"><p id="p1453804585415"><a name="p1453804585415"></a><a name="p1453804585415"></a>父类</p>
    </th>
    <th class="cellrowborder" valign="top" width="66.93%" id="mcps1.2.4.1.3"><p id="p3538164519546"><a name="p3538164519546"></a><a name="p3538164519546"></a>导入语句</p>
    </th>
    </tr>
    </thead>
    <tbody><tr id="row175381645175416"><td class="cellrowborder" valign="top" width="13.05%" headers="mcps1.2.4.1.1 "><p id="p18538445155411"><a name="p18538445155411"></a><a name="p18538445155411"></a>TensorFlow</p>
    </td>
    <td class="cellrowborder" valign="top" width="20.02%" headers="mcps1.2.4.1.2 "><p id="p653815455548"><a name="p653815455548"></a><a name="p653815455548"></a>TfServingBaseService</p>
    </td>
    <td class="cellrowborder" valign="top" width="66.93%" headers="mcps1.2.4.1.3 "><p id="p2538124516544"><a name="p2538124516544"></a><a name="p2538124516544"></a>from model_service.tfserving_model_service import TfServingBaseService</p>
    </td>
    </tr>
    <tr id="row105381145195413"><td class="cellrowborder" valign="top" width="13.05%" headers="mcps1.2.4.1.1 "><p id="p753884545414"><a name="p753884545414"></a><a name="p753884545414"></a>MXNet</p>
    </td>
    <td class="cellrowborder" valign="top" width="20.02%" headers="mcps1.2.4.1.2 "><p id="p85381445185418"><a name="p85381445185418"></a><a name="p85381445185418"></a>MXNetBaseService</p>
    </td>
    <td class="cellrowborder" valign="top" width="66.93%" headers="mcps1.2.4.1.3 "><p id="p953813457540"><a name="p953813457540"></a><a name="p953813457540"></a>from mms.model_service.mxnet_model_service import MXNetBaseService</p>
    </td>
    </tr>
    <tr id="row205381745145415"><td class="cellrowborder" valign="top" width="13.05%" headers="mcps1.2.4.1.1 "><p id="p8538164555417"><a name="p8538164555417"></a><a name="p8538164555417"></a>PyTorch</p>
    </td>
    <td class="cellrowborder" valign="top" width="20.02%" headers="mcps1.2.4.1.2 "><p id="p1453811458545"><a name="p1453811458545"></a><a name="p1453811458545"></a>PTServingBaseService</p>
    </td>
    <td class="cellrowborder" valign="top" width="66.93%" headers="mcps1.2.4.1.3 "><p id="p105381445125414"><a name="p105381445125414"></a><a name="p105381445125414"></a>from model_service.pytorch_model_service import PTServingBaseService</p>
    </td>
    </tr>
    <tr id="row19420104110717"><td class="cellrowborder" valign="top" width="13.05%" headers="mcps1.2.4.1.1 "><p id="p34201441972"><a name="p34201441972"></a><a name="p34201441972"></a>Pyspark</p>
    </td>
    <td class="cellrowborder" valign="top" width="20.02%" headers="mcps1.2.4.1.2 "><p id="p842116419713"><a name="p842116419713"></a><a name="p842116419713"></a>SparkServingBaseService</p>
    </td>
    <td class="cellrowborder" valign="top" width="66.93%" headers="mcps1.2.4.1.3 "><p id="p206411101888"><a name="p206411101888"></a><a name="p206411101888"></a>from model_service.spark_model_service import SparkServingBaseService</p>
    </td>
    </tr>
    <tr id="row62955965118"><td class="cellrowborder" valign="top" width="13.05%" headers="mcps1.2.4.1.1 "><p id="p1329613925117"><a name="p1329613925117"></a><a name="p1329613925117"></a>Caffe</p>
    </td>
    <td class="cellrowborder" valign="top" width="20.02%" headers="mcps1.2.4.1.2 "><p id="p132965918515"><a name="p132965918515"></a><a name="p132965918515"></a>CaffeBaseService</p>
    </td>
    <td class="cellrowborder" valign="top" width="66.93%" headers="mcps1.2.4.1.3 "><p id="p829619965117"><a name="p829619965117"></a><a name="p829619965117"></a>from model_service.caffe_model_service import CaffeBaseService</p>
    </td>
    </tr>
    <tr id="row17610103452920"><td class="cellrowborder" valign="top" width="13.05%" headers="mcps1.2.4.1.1 "><p id="p148249131016"><a name="p148249131016"></a><a name="p148249131016"></a>XGBoost</p>
    </td>
    <td class="cellrowborder" valign="top" width="20.02%" headers="mcps1.2.4.1.2 "><p id="p182411131007"><a name="p182411131007"></a><a name="p182411131007"></a>XgSklServingBaseService</p>
    </td>
    <td class="cellrowborder" valign="top" width="66.93%" headers="mcps1.2.4.1.3 "><p id="p182415131101"><a name="p182415131101"></a><a name="p182415131101"></a>from model_service.python_model_service import XgSklServingBaseService</p>
    </td>
    </tr>
    <tr id="row11786153892914"><td class="cellrowborder" valign="top" width="13.05%" headers="mcps1.2.4.1.1 "><p id="p347261718013"><a name="p347261718013"></a><a name="p347261718013"></a>Scikit_Learn</p>
    </td>
    <td class="cellrowborder" valign="top" width="20.02%" headers="mcps1.2.4.1.2 "><p id="p647251710016"><a name="p647251710016"></a><a name="p647251710016"></a>XgSklServingBaseService</p>
    </td>
    <td class="cellrowborder" valign="top" width="66.93%" headers="mcps1.2.4.1.3 "><p id="p204721175016"><a name="p204721175016"></a><a name="p204721175016"></a>from model_service.python_model_service import XgSklServingBaseService</p>
    </td>
    </tr>
    </tbody>
    </table>

2.  可以重写的方法有以下几种。

    **表 2**  重写方法

    <a name="table119897712529"></a>
    <table><thead align="left"><tr id="row18990187175216"><th class="cellrowborder" valign="top" width="26.36%" id="mcps1.2.3.1.1"><p id="p79906795211"><a name="p79906795211"></a><a name="p79906795211"></a>方法名</p>
    </th>
    <th class="cellrowborder" valign="top" width="73.64%" id="mcps1.2.3.1.2"><p id="p59901773525"><a name="p59901773525"></a><a name="p59901773525"></a>说明</p>
    </th>
    </tr>
    </thead>
    <tbody><tr id="row1899007155210"><td class="cellrowborder" valign="top" width="26.36%" headers="mcps1.2.3.1.1 "><p id="p999087135218"><a name="p999087135218"></a><a name="p999087135218"></a>__init__(self, model_name, model_path)</p>
    </td>
    <td class="cellrowborder" valign="top" width="73.64%" headers="mcps1.2.3.1.2 "><p id="p1799014717525"><a name="p1799014717525"></a><a name="p1799014717525"></a>初始化方法，该方法内加载模型及标签等（pytorch和caffe类型模型必须重写，实现模型加载逻辑）。</p>
    </td>
    </tr>
    <tr id="row20990187165211"><td class="cellrowborder" valign="top" width="26.36%" headers="mcps1.2.3.1.1 "><p id="p4990177155212"><a name="p4990177155212"></a><a name="p4990177155212"></a>_preprocess(self, data)</p>
    </td>
    <td class="cellrowborder" valign="top" width="73.64%" headers="mcps1.2.3.1.2 "><p id="p2990171521"><a name="p2990171521"></a><a name="p2990171521"></a>预处理方法，在推理请求前调用，用于将API接口用户原始请求数据转换为模型期望输入数据。</p>
    </td>
    </tr>
    <tr id="row79900716522"><td class="cellrowborder" valign="top" width="26.36%" headers="mcps1.2.3.1.1 "><p id="p1199097165217"><a name="p1199097165217"></a><a name="p1199097165217"></a>_inference(self, data)</p>
    </td>
    <td class="cellrowborder" valign="top" width="73.64%" headers="mcps1.2.3.1.2 "><p id="p209905755218"><a name="p209905755218"></a><a name="p209905755218"></a>实际推理请求方法（不建议重写，重写后会覆盖modelarts内置的推理过程，运行自定义的推理逻辑）</p>
    </td>
    </tr>
    <tr id="row1499097145219"><td class="cellrowborder" valign="top" width="26.36%" headers="mcps1.2.3.1.1 "><p id="p19901173520"><a name="p19901173520"></a><a name="p19901173520"></a>_postprocess(self, data)</p>
    </td>
    <td class="cellrowborder" valign="top" width="73.64%" headers="mcps1.2.3.1.2 "><p id="p1990157165215"><a name="p1990157165215"></a><a name="p1990157165215"></a>后处理方法，在推理请求完成后调用，用于将模型输出转换为API接口输出</p>
    </td>
    </tr>
    </tbody>
    </table>

    >![](public_sys-resources/icon-note.gif) **说明：**   
    >-   通常，用户可以选择重写preprocess和postprocess方法，以实现数据的API输入的预处理和推理结果输出的后处理。  
    >-   重写BaseService继承类的初始化方法init可能导致模型“运行异常“  

3.  当前支持两种content-type的接口传入，“multipart/form-data“和“application/json“。
    -   “multipart/form-data“请求

        ```
        curl -X POST \
          <modelarts-inference-endpoint> \
          -F image1=@cat.jpg \
          -F images2=@horse.jpg
        ```

        对应的传入data为

        ```
        [
           {
              "image1":{
                 "cat.jpg":"<cat..jpg file io>"
              }
           },
           {
              "image2":{
                 "horse.jpg":"<horse.jpg file io>"
              }
           }
        ]
        ```

    -   “application/json“请求

        ```
         curl -X POST \
           <modelarts-inference-endpoint> \
           -d '{
            "images":"base64 encode image"
            }'
        ```

        对应的传入data为python dict

        ```
         {
            "images":"base64 encode image"
        
         }
        ```

4.  可以使用的属性为模型所在的本地目录，属性名为“self.model\_path“。另外pyspark模型在“customize\_service.py“中可以使用“self.spark“获取SparkSession对象。

## TensorFlow的推理脚本示例<a name="section0900184692811"></a>

TensorFlow MnistService示例如下。

-   推理代码

    ```
    from PIL import Image
    import numpy as np
    from model_service.tfserving_model_service import TfServingBaseService
    
    class mnist_service(TfServingBaseService):
    
        def _preprocess(self, data):
            preprocessed_data = {}
    
            for k, v in data.items():
                for file_name, file_content in v.items():
                    image1 = Image.open(file_content)
                    image1 = np.array(image1, dtype=np.float32)
                    image1.resize((1, 784))
                    preprocessed_data[k] = image1
    
            return preprocessed_data
    
        def _postprocess(self, data):
    
            infer_output = {}
    
            for output_name, result in data.items():
    
                infer_output["mnist_result"] = result[0].index(max(result[0]))
    
            return infer_output
    ```

-   请求

    ```
    curl -X POST \ 在线服务地址 \ -F images=@test.jpg
    ```

-   返回

    ```
    {"mnist_result": 7}
    ```


在上面的代码示例中，完成了将用户表单输入的图片的大小调整，转换为可以适配模型输入的shape。首先通过Pillow库读取“32×32“的图片，调整图片大小为“1×784“以匹配模型输入。在后续处理中，转换模型输出为列表，用于Restful接口输出展示。

## 自定义推理逻辑的推理脚本示例<a name="section971394732916"></a>

首先，需要在配置文件中，定义自己的依赖包，详细示例请参见[使用自定义依赖包的模型配置文件示例](模型配置文件编写说明.md#section119911955122011)。然后通过如下所示示例代码，实现了“saved\_model“格式模型的加载推理。

```
# -*- coding: utf-8 -*-
import json
import os
import threading

import numpy as np
import tensorflow as tf
from PIL import Image

from model_service.tfserving_model_service import TfServingBaseService
import logging

logger = logging.getLogger(__name__)


class MnistService(TfServingBaseService):

    def __init__(self, model_name, model_path):
        self.model_name = model_name
        self.model_path = model_path
        self.model_inputs = {}
        self.model_outputs = {}

        # label文件可以在这里加载,在后处理函数里使用
        # label.txt放在OBS和模型包的目录

        # with open(os.path.join(self.model_path, 'label.txt')) as f:
        #     self.label = json.load(f)

        # 非阻塞方式加载saved_model模型，防止阻塞超时
        thread = threading.Thread(target=self.get_tf_sess)
        thread.start()

    def get_tf_sess(self):
        # 加载saved_model格式的模型

        # session要重用，建议不要用with语句
        sess = tf.Session(graph=tf.Graph())
        meta_graph_def = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], self.model_path)
        signature_defs = meta_graph_def.signature_def

        self.sess = sess

        signature = []

        # only one signature allowed
        for signature_def in signature_defs:
            signature.append(signature_def)
        if len(signature) == 1:
            model_signature = signature[0]
        else:
            logger.warning("signatures more than one, use serving_default signature")
            model_signature = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY

        logger.info("model signature: %s", model_signature)

        for signature_name in meta_graph_def.signature_def[model_signature].inputs:
            tensorinfo = meta_graph_def.signature_def[model_signature].inputs[signature_name]
            name = tensorinfo.name
            op = self.sess.graph.get_tensor_by_name(name)
            self.model_inputs[signature_name] = op

        logger.info("model inputs: %s", self.model_inputs)

        for signature_name in meta_graph_def.signature_def[model_signature].outputs:
            tensorinfo = meta_graph_def.signature_def[model_signature].outputs[signature_name]
            name = tensorinfo.name
            op = self.sess.graph.get_tensor_by_name(name)

            self.model_outputs[signature_name] = op

        logger.info("model outputs: %s", self.model_outputs)

    def _preprocess(self, data):
        # https两种请求形式
        # 1. form-data文件格式的请求对应：data = {"请求key值":{"文件名":<文件io>}}
        # 2. json格式对应：data = json.loads("接口传入的json体")
        preprocessed_data = {}

        for k, v in data.items():
            for file_name, file_content in v.items():
                image1 = Image.open(file_content)
                image1 = np.array(image1, dtype=np.float32)
                image1.resize((1, 28, 28))
                preprocessed_data[k] = image1

        return preprocessed_data

    def _inference(self, data):

        feed_dict = {}
        for k, v in data.items():
            if k not in self.model_inputs.keys():
                logger.error("input key %s is not in model inputs %s", k, list(self.model_inputs.keys()))
                raise Exception("input key %s is not in model inputs %s" % (k, list(self.model_inputs.keys())))
            feed_dict[self.model_inputs[k]] = v

        result = self.sess.run(self.model_outputs, feed_dict=feed_dict)
        logger.info('predict result : ' + str(result))

        return result

    def _postprocess(self, data):
        infer_output = {"mnist_result": []}
        for output_name, results in data.items():

            for result in results:
                infer_output["mnist_result"].append(np.argmax(result))

        return infer_output

    def __del__(self):
        self.sess.close()
```

